#!/bin/bash
# Engram with Ollama - Direct Launcher with FAISS vector database
# This script directly patches the memory system to use FAISS and launches Ollama

set -e  # Exit on any error

# Get the directory where the script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Default values
MODEL="llama3:8b"
PROMPT_TYPE="combined"
CLIENT_ID="ollama"
TEMPERATURE="0.7"
MAX_TOKENS="2048"
MEMORY_FUNCTIONS=true
AVAILABLE_MODELS="Claude"

# Parse command-line arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --model)
      MODEL="$2"
      shift 2
      ;;
    --prompt-type)
      PROMPT_TYPE="$2"
      shift 2
      ;;
    --client-id)
      CLIENT_ID="$2"
      shift 2
      ;;
    --temperature)
      TEMPERATURE="$2"
      shift 2
      ;;
    --max-tokens)
      MAX_TOKENS="$2"
      shift 2
      ;;
    --no-memory)
      MEMORY_FUNCTIONS=false
      shift
      ;;
    --available-models)
      AVAILABLE_MODELS="$2"
      shift 2
      ;;
    --help)
      echo "Engram with Ollama - Direct launcher with FAISS vector database"
      echo ""
      echo "Usage: engram_with_ollama_direct [options]"
      echo ""
      echo "Options:"
      echo "  --model MODEL             Ollama model to use (default: llama3:8b)"
      echo "  --prompt-type TYPE        System prompt type: memory, communication, combined (default: combined)"
      echo "  --client-id ID            Client ID for Engram (default: ollama)"
      echo "  --temperature TEMP        Temperature for generation (default: 0.7)"
      echo "  --max-tokens TOKENS       Maximum tokens to generate (default: 2048)"
      echo "  --no-memory               Disable memory functions"
      echo "  --available-models MODELS Space-separated list of available models (default: Claude)"
      echo "  --help                    Show this help message"
      exit 0
      ;;
    *)
      echo "Unknown option: $1"
      echo "Use --help to see available options"
      exit 1
      ;;
  esac
done

# Check for dependencies
if ! pip show faiss-cpu >/dev/null 2>&1; then
  echo "Installing FAISS and dependencies..."
  pip install faiss-cpu numpy>=2.0.0 sentence-transformers>=2.2.2
fi

# Store the current working directory
CURRENT_DIR="$(pwd)"

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
  echo "Error: Ollama is not running. Please start Ollama first."
  exit 1
fi

# Apply the memory patch
echo "Applying FAISS memory patch to Engram..."
MEMORY_PATCH_PATH="$SCRIPT_DIR/utils/engram_memory_patch.py"
if [ -f "$MEMORY_PATCH_PATH" ]; then
  python "$MEMORY_PATCH_PATH"
else
  echo "Warning: Memory patch not found at $MEMORY_PATCH_PATH"
  echo "Continuing without applying the patch..."
fi

# Use absolute paths for Ollama bridge script
OLLAMA_BRIDGE="$SCRIPT_DIR/ollama_bridge.py"

# Verify the bridge file exists
if [ ! -f "$OLLAMA_BRIDGE" ]; then
  echo "Error: Ollama bridge script not found at $OLLAMA_BRIDGE"
  echo "Please check the installation path or reinstall Engram"
  exit 1
fi

# Build command with full absolute paths for scripts but preserving current directory
CMD="python $OLLAMA_BRIDGE $MODEL --prompt-type $PROMPT_TYPE --client-id $CLIENT_ID --temperature $TEMPERATURE --max-tokens $MAX_TOKENS"

if [ "$MEMORY_FUNCTIONS" = true ]; then
  CMD="$CMD --memory-functions"
fi

for model in $AVAILABLE_MODELS; do
  CMD="$CMD --available-models $model"
done

# Start the Ollama bridge
echo "Starting Ollama bridge with Engram memory (FAISS-enabled)..."
echo "Model: $MODEL"
echo "Prompt type: $PROMPT_TYPE"
echo "Client ID: $CLIENT_ID"
echo "Working directory: $CURRENT_DIR"
echo ""
echo "Type 'exit' or '/quit' to exit"
echo ""

# Execute the command from the current directory
exec $CMD