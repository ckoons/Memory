#!/bin/bash
# engram_with_ollama_hermes
# Launch Ollama with Engram memory services, Hermes integration, and smart vector database detection
# Created: March 30, 2025

# Default values
MODEL="llama3:8b"
PROMPT_TYPE="combined"
CLIENT_ID="ollama"
TEMPERATURE="0.7"
MAX_TOKENS="2048"
MEMORY_FUNCTIONS=true
AVAILABLE_MODELS="Claude"
USE_SMART_DETECTION=true

# ANSI color codes for terminal output
BLUE="\033[94m"
GREEN="\033[92m"
YELLOW="\033[93m"
RED="\033[91m"
BOLD="\033[1m"
RESET="\033[0m"

# Parse command-line arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --model)
      MODEL="$2"
      shift 2
      ;;
    --prompt-type)
      PROMPT_TYPE="$2"
      shift 2
      ;;
    --client-id)
      CLIENT_ID="$2"
      shift 2
      ;;
    --temperature)
      TEMPERATURE="$2"
      shift 2
      ;;
    --max-tokens)
      MAX_TOKENS="$2"
      shift 2
      ;;
    --no-memory)
      MEMORY_FUNCTIONS=false
      shift
      ;;
    --available-models)
      AVAILABLE_MODELS="$2"
      shift 2
      ;;
    --fallback)
      USE_SMART_DETECTION=false
      export ENGRAM_USE_FALLBACK=1
      shift
      ;;
    --help)
      echo "Engram with Ollama & Hermes - Launcher script for Ollama integration with Hermes services"
      echo ""
      echo "Usage: engram_with_ollama_hermes [options]"
      echo ""
      echo "Options:"
      echo "  --model MODEL             Ollama model to use (default: llama3:8b)"
      echo "  --prompt-type TYPE        System prompt type: memory, communication, combined (default: combined)"
      echo "  --client-id ID            Client ID for Engram (default: ollama)"
      echo "  --temperature TEMP        Temperature for generation (default: 0.7)"
      echo "  --max-tokens TOKENS       Maximum tokens to generate (default: 2048)"
      echo "  --no-memory               Disable memory functions"
      echo "  --available-models MODELS Space-separated list of available models (default: Claude)"
      echo "  --fallback                Use file-based memory (no vector database)"
      echo "  --help                    Show this help message"
      exit 0
      ;;
    *)
      echo "Unknown option: $1"
      echo "Use --help to see available options"
      exit 1
      ;;
  esac
done

# Store the current working directory to maintain it
CURRENT_DIR="$(pwd)"

# Get the directory where the script is located
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$SCRIPT_DIR"

# Find the Engram installation directory
# First try to find it using pip
ENGRAM_PKG_DIR=$(python -c "import engram; print(engram.__path__[0])" 2>/dev/null)
if [ -n "$ENGRAM_PKG_DIR" ]; then
  # Go up one level from the package directory to get the installation directory
  ENGRAM_DIR=$(dirname "$ENGRAM_PKG_DIR")
  echo -e "${GREEN}Found Engram installation via pip at: $ENGRAM_DIR${RESET}"
else
  # Use our current directory
  ENGRAM_DIR="$SCRIPT_DIR/.."
  echo -e "${YELLOW}Using current directory for Engram: $ENGRAM_DIR${RESET}"
fi

# Add Hermes directory to Python path
HERMES_DIR=$(cd "$SCRIPT_DIR/../../Hermes" && pwd)
if [ -d "$HERMES_DIR" ]; then
  echo -e "${GREEN}Found Hermes at: $HERMES_DIR${RESET}"
  export PYTHONPATH="$HERMES_DIR:$PYTHONPATH"
else
  echo -e "${YELLOW}Hermes directory not found at $HERMES_DIR${RESET}"
  echo -e "${YELLOW}Proceeding without Hermes integration${RESET}"
fi

# Make helper scripts executable
chmod +x "$SCRIPT_DIR/../utils/detect_best_vector_db.py" 2>/dev/null

# Detect best vector database if smart detection is enabled
if [ "$USE_SMART_DETECTION" = true ]; then
    echo -e "${BLUE}Detecting optimal vector database...${RESET}"
    
    if [ -f "$SCRIPT_DIR/../utils/detect_best_vector_db.py" ]; then
        # Run detection script to determine the best DB
        DB_INFO=$(python "$SCRIPT_DIR/../utils/detect_best_vector_db.py" --quiet)
        DB_AVAILABLE=$?
        
        if [ $DB_AVAILABLE -eq 0 ] && [ -n "$DB_INFO" ]; then
            # Extract the database name from the path
            DB_NAME=$(basename "$DB_INFO" | sed 's/engram_with_//' | sed 's/_ollama//' | sed 's/_faiss//' | sed 's/_lancedb//')
            
            if [[ "$DB_INFO" == *"lancedb"* ]]; then
                echo -e "${GREEN}Using LanceDB vector database (optimal for your hardware)${RESET}"
                export ENGRAM_USE_VECTORDB=lancedb
                export ENGRAM_USE_FALLBACK=0
            elif [[ "$DB_INFO" == *"faiss"* ]]; then
                echo -e "${GREEN}Using FAISS vector database (optimal for your hardware)${RESET}"
                export ENGRAM_USE_VECTORDB=faiss
                export ENGRAM_USE_FALLBACK=0
            else
                echo -e "${YELLOW}No optimal vector database detected, using Hermes database services${RESET}"
                export ENGRAM_USE_HERMES=1
                export ENGRAM_USE_FALLBACK=0
            fi
        else
            echo -e "${YELLOW}Vector database detection failed, using Hermes database services${RESET}"
            export ENGRAM_USE_HERMES=1
            export ENGRAM_USE_FALLBACK=0
        fi
    else
        echo -e "${YELLOW}Vector database detection script not found, using Hermes database services${RESET}"
        export ENGRAM_USE_HERMES=1
        export ENGRAM_USE_FALLBACK=0
    fi
    
    # For LanceDB, set up hardware-specific optimizations
    if [ "$ENGRAM_USE_VECTORDB" = "lancedb" ]; then
        # Check for Apple Silicon for Metal optimizations
        if [[ "$(uname -m)" == "arm64" ]] && [[ "$(uname -s)" == "Darwin" ]]; then
            echo -e "${GREEN}Running on Apple Silicon, optimizing for Metal...${RESET}"
            export LANCEDB_USE_METAL=1
        fi

        # Check for CUDA for optimizations
        CUDA_AVAILABLE=$(python -c "import torch; print(torch.cuda.is_available())" 2>/dev/null)
        if [ "$CUDA_AVAILABLE" == "True" ]; then
            echo -e "${GREEN}CUDA detected, enabling GPU acceleration...${RESET}"
            export LANCEDB_USE_CUDA=1
        fi
    fi
fi

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
  echo -e "${RED}Error: Ollama is not running. Please start Ollama first.${RESET}"
  exit 1
fi

# Use absolute paths for Ollama bridge script
OLLAMA_BRIDGE="$SCRIPT_DIR/ollama_bridge.py"
echo -e "${BLUE}Using Ollama bridge: $OLLAMA_BRIDGE${RESET}"

# Verify the bridge file exists
if [ ! -f "$OLLAMA_BRIDGE" ]; then
  echo -e "${RED}Error: Ollama bridge script not found at $OLLAMA_BRIDGE${RESET}"
  echo -e "${RED}Please check the installation path or reinstall Engram${RESET}"
  exit 1
fi

# Add paths to PYTHONPATH if needed
export PYTHONPATH="$SCRIPT_DIR:$ENGRAM_DIR:$PYTHONPATH"

# Register Ollama with Hermes service registry - using inline Python for simplicity
if [ -d "$HERMES_DIR" ]; then
    echo -e "${BLUE}Registering Ollama with Hermes service registry...${RESET}"
    
    python -c "
import sys, os
import asyncio

try:
    sys.path.insert(0, '$HERMES_DIR')
    from hermes.core.service_discovery import ServiceRegistry
    
    async def register_with_hermes():
        try:
            # Initialize service registry
            registry = ServiceRegistry()
            await registry.start()
            
            # Register Ollama with Hermes
            success = await registry.register(
                service_id=f'ollama-$CLIENT_ID',
                name=f'Ollama ($CLIENT_ID)',
                version='latest',
                endpoint='http://localhost:11434',
                capabilities=['llm', 'reasoning', 'embedding'],
                metadata={
                    'client_id': '$CLIENT_ID',
                    'model': '$MODEL',
                    'provider': 'ollama'
                }
            )
            
            if success:
                print('\033[92mRegistered Ollama ($CLIENT_ID) with Hermes service registry\033[0m')
            else:
                print('\033[93mFailed to register Ollama with Hermes\033[0m')
        except Exception as e:
            print(f'\033[93mError registering with Hermes: {e}\033[0m')
    
    # Run the registration
    asyncio.run(register_with_hermes())
except Exception as e:
    print(f'\033[93mError importing Hermes modules: {e}\033[0m')
"
fi

# Build command with full absolute paths for scripts but preserving current directory
CMD="python $OLLAMA_BRIDGE $MODEL --prompt-type $PROMPT_TYPE --client-id $CLIENT_ID --temperature $TEMPERATURE --max-tokens $MAX_TOKENS"

if [ "$MEMORY_FUNCTIONS" = true ]; then
  CMD="$CMD --memory-functions"
fi

for model in $AVAILABLE_MODELS; do
  CMD="$CMD --available-models $model"
done

# Add Hermes integration if available
if [ -d "$HERMES_DIR" ] && [ "$ENGRAM_USE_HERMES" = "1" ]; then
  CMD="$CMD --hermes-integration"
fi

# Start the Ollama bridge
echo -e "${GREEN}${BOLD}Starting Ollama bridge with Engram memory and Hermes integration...${RESET}"
echo -e "${BLUE}Model:${RESET} $MODEL"
echo -e "${BLUE}Prompt type:${RESET} $PROMPT_TYPE"
echo -e "${BLUE}Client ID:${RESET} $CLIENT_ID"
echo -e "${BLUE}Working directory:${RESET} $CURRENT_DIR"

# Display memory system information
if [ "$ENGRAM_USE_FALLBACK" = "1" ]; then
    echo -e "${YELLOW}Memory:${RESET} File-based (fallback mode)"
elif [ "$ENGRAM_USE_HERMES" = "1" ]; then
    echo -e "${GREEN}Memory:${RESET} Hermes centralized database services"
else
    if [ "$ENGRAM_USE_VECTORDB" = "lancedb" ]; then
        echo -e "${GREEN}Memory:${RESET} LanceDB vector database"
    else
        echo -e "${GREEN}Memory:${RESET} FAISS vector database"
    fi
fi

echo ""
echo -e "${BLUE}Type 'exit' or '/quit' to exit${RESET}"
echo ""

# Execute the command from the current directory
# This ensures the bridge sees the files in the user's current directory
cd "$CURRENT_DIR"
exec $CMD